{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "164f9270-55f6-4371-bdc6-79dad175b37d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Setup and Spark Basics\n",
    "\n",
    "Ing. Jeison Robles Arias\n",
    "\n",
    "```Goal```: learn transformations vs actions, schema, explain plans, partitions, cache, basic I/O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65991195-b8d4-4571-978f-f4a7ee16d377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#import spark\n",
    "#import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e5dd087-a5d0-4d4f-b844-07fa85b0a6b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Confirm working\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b471f18-38af-4063-8727-eb0fc7fb3d79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5de008ec-8748-4556-b620-1f6bc1ced958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b129ce09-8aab-4ec1-a18d-6564e8cdc800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create an Small DataFrame with explicit schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cc526d3-6576-4d68-9d79-42411a2e1dd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, \"alice\", \"CR\", 120.50),\n",
    "    (2, \"bob\", \"CR\", 75.00),\n",
    "    (3, \"carol\", \"PA\", 210.10),\n",
    "    (4, \"dave\", \"CR\", 10.00),\n",
    "    (5, \"erin\", \"PA\", 99.99),\n",
    "]\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"customer_id\", T.IntegerType(), False),\n",
    "    T.StructField(\"name\", T.StringType(), True),\n",
    "    T.StructField(\"country\", T.StringType(), True),\n",
    "    T.StructField(\"amount\", T.DoubleType(), True),\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80a23c2d-9461-4a83-a23d-a2f532f1e49f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transformations vs Actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7669bd9d-9983-45a8-8300-72ebd4d0f99d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cr = df.filter(F.col(\"country\") == \"CR\").select(\"customer_id\",\"amount\")\n",
    "df_cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4cc72f0-45e3-48e3-adec-d3c8ee06fad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Trigger an action (runs a Spark Job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00014410-2c4b-4c6b-8de7-4f70fdd0408e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cr.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81a231a6-d718-4db1-9738-b6b846347aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Common DataFrame Operations\n",
    "\n",
    "with column + select + orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77867a5b-245f-4630-b48e-3a3cbeebb198",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = (\n",
    "    df\n",
    "    .withColumn(\"amount_usd\", F.round(F.col(\"amount\"), 2)) # Creates a new column\n",
    "    .withColumn(\"is_high\", F.col(\"amount\")  >= F.lit(100.0)) # Booolean\n",
    "    .select(\"customer_id\",\"country\",\"amount\",\"amount_usd\",\"is_high\")\n",
    "    .orderBy(F.desc(\"amount_usd\"))\n",
    ")\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5e57637-3f34-43ac-bbe2-5e85854f223c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "agg = (\n",
    "    df\n",
    "    .groupBy(\"country\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"n_rows\"),    # alias it just for adding names\n",
    "        F.countDistinct(\"customer_id\").alias(\"n_customers\"),\n",
    "        F.round(F.sum(\"amount\"),2).alias(\"total_amount\"),\n",
    "        F.round(F.avg(\"amount\"),2).alias(\"avg_amount\"),\n",
    "        F.max(\"amount\").alias(\"max_amount\")\n",
    "    )\n",
    "    .orderBy(F.desc(\"total_amount\"))\n",
    ")\n",
    "\n",
    "display(agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbc24fa8-9e9a-42ad-a5b8-70a4623987ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "___\n",
    "## Inspect the plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71bf80a1-67a6-4f04-81cf-03e08bb56bba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "agg.explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df729799-4a17-4d0a-b7a3-7ca5c48c9c74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Partitions:\" , df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52b6934d-b6cc-4c27-92eb-e4a6cfb27b29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_big = spark.range(0, 500_000).withColumn(\"x\", (F.col(\"id\") % 100).cast(\"int\"))\n",
    "\n",
    "# Without cache: Spark recomputes df_big each time\n",
    "_ = df_big.groupBy(\"x\").count().count()\n",
    "_ = df_big.groupBy(\"x\").agg(F.avg(\"id\")).count()\n",
    "\n",
    "# With cache: compute once, reuse\n",
    "df_big_cached = df_big.cache()\n",
    "_ = df_big_cached.count()  # materialize cache\n",
    "\n",
    "_ = df_big_cached.groupBy(\"x\").count().count()\n",
    "_ = df_big_cached.groupBy(\"x\").agg(F.avg(\"id\")).count()\n",
    "\n",
    "df_big_cached.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06571049-1bdd-499d-92ba-1e52e7f4c79d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tmp_path = \"dbfs:/tmp/spark_lakehouse_labs/lab00_parquet_df\"\n",
    "\n",
    "(df2\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .parquet(tmp_path)\n",
    ")\n",
    "\n",
    "df_read = spark.read.parquet(tmp_path)\n",
    "display(df_read)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_setup_and_spark_basics copy",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
