{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dcf3fbb-28f7-4dd8-ad84-456c328aa5b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lab 01 - Spark DataFrames Deep Dive\n",
    "\n",
    "## Objetive:\n",
    "\n",
    "Understand how Spark DataFrames work internally and how to manipulate them efficiently:\n",
    "\n",
    "- Schema inference vs explicit schema\n",
    "- Column expressions\n",
    "- Transformations vs actions (deep view)\n",
    "- Filtering & projections\n",
    "- Derived columns\n",
    "- Null handling\n",
    "- Aggregations\n",
    "- Window functions\n",
    "- Execution plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d363f035-bcb8-4b15-bd62-abf32c81dc90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "___\n",
    "```Section 1``` — Loading Data the Right Way\n",
    "\n",
    "In real pipelines, bad schema inference causes:\n",
    "- Wrong types\n",
    "- Slow reads\n",
    "- Broken aggregations\n",
    "- Unexpected casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84580469-7ac4-49b6-b60f-cffee922fbfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Create a sample DataSet\n",
    "# =======================\n",
    "data = [\n",
    "    (1, \"2026-01-01\", \"card\", 120.50, \"CR\"),\n",
    "    (2, \"2026-01-01\", \"transfer\", 540.00, \"CR\"),\n",
    "    (3, \"2026-01-02\", \"card\", 75.25, \"US\"),\n",
    "    (4, \"2026-01-02\", \"card\", None, \"CR\"),\n",
    "]\n",
    "print(type(data))\n",
    "\n",
    "# =======================\n",
    "# Create an Explicit Schema \n",
    "# =======================\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), False), #(name, data type, nullable)\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"channel\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "])\n",
    "\n",
    "# =======================\n",
    "# Feeding the df\n",
    "# =======================\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52abad09-87d8-46b8-9400-b8b7ad34ba6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92a7d4e3-e9a1-4c88-8a94-13642460ba90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Some comentaries\n",
    "- Spark does not validate schema strictly unless enforced.\n",
    "- Schema affects ```Catalyst*``` optimization.\n",
    "- Explicit schema is faster than inference for large files.\n",
    "- String date is not ideal → we will cast.\n",
    "\n",
    "\n",
    "**Note**:\n",
    "Catalyst is the query optimizer of Apache Spark. Basically decides \n",
    "- When to apply filters\n",
    "- How to reorder operations\n",
    "- Wheter to push filters down \n",
    "- wheter to eliminate unused columns\n",
    "- Wheter to optimize joins\n",
    "\n",
    "Catalyst builds **the most efficient physical plan**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "532a6dd9-2731-4ebd-8d6e-f949cd28106d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.version"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_dataframes_deep_dive",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
