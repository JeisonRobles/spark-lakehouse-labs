{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dcf3fbb-28f7-4dd8-ad84-456c328aa5b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lab 01 - Spark DataFrames Deep Dive\n",
    "\n",
    "## Objetive:\n",
    "\n",
    "Understand how Spark DataFrames work internally and how to manipulate them efficiently:\n",
    "\n",
    "- Schema inference vs explicit schema\n",
    "- Column expressions\n",
    "- Transformations vs actions (deep view)\n",
    "- Filtering & projections\n",
    "- Derived columns\n",
    "- Null handling\n",
    "- Aggregations\n",
    "- Window functions\n",
    "- Execution plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d363f035-bcb8-4b15-bd62-abf32c81dc90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "___\n",
    "```Section 1``` — Loading Data the Right Way\n",
    "\n",
    "In real pipelines, bad schema inference causes:\n",
    "- Wrong types\n",
    "- Slow reads\n",
    "- Broken aggregations\n",
    "- Unexpected casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84580469-7ac4-49b6-b60f-cffee922fbfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Create a sample DataSet\n",
    "# =======================\n",
    "data = [\n",
    "    (1, \"2026-01-01\", \"card\", 120.50, \"CR\"),\n",
    "    (2, \"2026-01-01\", \"transfer\", 540.00, \"CR\"),\n",
    "    (3, \"2026-01-02\", \"card\", 75.25, \"US\"),\n",
    "    (4, \"2026-01-02\", \"card\", None, \"CR\"),\n",
    "]\n",
    "print(type(data))\n",
    "\n",
    "# =======================\n",
    "# Create an Explicit Schema \n",
    "# =======================\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), False), #(name, data type, nullable)\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"channel\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "])\n",
    "\n",
    "# =======================\n",
    "# Feeding the df\n",
    "# =======================\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52abad09-87d8-46b8-9400-b8b7ad34ba6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92a7d4e3-e9a1-4c88-8a94-13642460ba90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Some comentaries\n",
    "- Spark does not validate schema strictly unless enforced.\n",
    "- Schema affects ```Catalyst*``` optimization.\n",
    "- Explicit schema is faster than inference for large files.\n",
    "- String date is not ideal → we will cast.\n",
    "\n",
    "\n",
    "**Note**:\n",
    "Catalyst is the query optimizer of Apache Spark. Basically decides \n",
    "- When to apply filters\n",
    "- How to reorder operations\n",
    "- Wheter to push filters down \n",
    "- wheter to eliminate unused columns\n",
    "- Wheter to optimize joins\n",
    "\n",
    "Catalyst builds **the most efficient physical plan**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4df9d8e0-e722-435a-9788-e0fa689a482c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```Section 2``` - Column Expressions\n",
    "\n",
    "When working with spark is important to avoid thinking with the row-level mindset(like pandas) and start thinking on column expressions mindset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e76a2a3-1b04-4e5a-a8b7-23d7aa139455",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"amount_with_tax\", col(\"amount\") * 1.13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b871ccd-dcf0-46e8-b9f0-d6993d08955c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "998ababa-cfd2-4dec-bbef-71443929dcaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```Section 3``` Transformations vs Actions (Deep)\n",
    "\n",
    "**Transformations:**\n",
    "- select\n",
    "- filter\n",
    "- withColumn\n",
    "- groupBy\n",
    "\n",
    "**Actions:**\n",
    "- show\n",
    "- collect\n",
    "- count\n",
    "- write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "579b29b5-96cd-4ddc-8b55-306037da6baa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filtered = df.filter(col(\"amount\") > 100) # Transformations do not show results (lazy evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27e413b8-7e87-48dc-b283-b8ed102fa50a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filtered.count() # Actions do show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a51de5cf-600a-4b53-a9f2-389a3a514ada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filtered.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8abc22ed-b9c7-4e16-8375-95262709212a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```Section 4``` Null Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96722e05-a11c-455c-a6c2-fcaeae15ab87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(col(\"amount\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "699ab9ba-cb3b-4814-b597-df456071a9e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(col(\"amount\").isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42b1a981-40e2-4938-aee9-3edb2ea37e79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Filling. NaN Values\n",
    "# =======================\n",
    "df.fillna({\"amount\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1d0e525-c6b4-4490-a466-c09bcf393994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "532a6dd9-2731-4ebd-8d6e-f949cd28106d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.version"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_dataframes_deep_dive",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
